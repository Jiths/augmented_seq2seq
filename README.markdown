# Augmented Seq2seq

![](https://img.shields.io/badge/status-wip-red.svg) ![](https://img.shields.io/badge/python-3.6-brightgreen.svg) ![](https://img.shields.io/badge/tensorflow-1.0.0-green.svg) ![](https://img.shields.io/badge/nltk-3.2.2-yellowgreen.svg)

- [x] Vanilla Seq2seq
- [ ] Conditioning decoder on external context
- [ ] Language Modeling with encoder
- [ ] Bidirectional encoder
- [ ] RNNSearch : Soft Alignment
- [ ] Memory augmentation
- [ ] Multi-turn Conversation Modeling with Hierarchical Recurrent Encoder-Decoder (HRED)

## Reference

1. Ed Grefenstette, *Beyond Sequence to Sequence with Augmented RNNs* [video](https://www.youtube.com/watch?v=4deLk3Eu05E), [slides](http://videolectures.net/site/normal_dl/tag=1051689/deeplearning2016_grefenstette_augmented_rnn_01.pdf)
2. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
3. [End-to-End Memory Networks](https://arxiv.org/abs/1503.08895)
4. [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://arxiv.org/abs/1507.04808)
